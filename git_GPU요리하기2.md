#### 📄 2강 상세 설명: 숨은 병목 찾기 - 데이터 전송 시간을 잡아라!

**목표:** GPU 컴퓨팅에서 실제 연산 시간보다 더 큰 성능 저하를 유발하는 '데이터 전송 오버헤드'의 심각성을 이해하고, NVIDIA Nsight Systems를 사용하여 병목 지점을 직접 확인하는 방법을 학습합니다.

**1. Host와 Device: 분리된 두 세계**

[![](images/Pasted%20image%2020250926155834.png)]

CUDA 프로그래밍 환경은 크게 두 부분으로 나뉩니다.

- **호스트 (Host):** CPU와 메인 메모리(DDR RAM)로 구성된 영역입니다. 프로그램의 전체 흐름을 제어하고, GPU에 작업을 지시하는 주체입니다.
    
- **디바이스 (Device):** GPU와 자체 비디오 메모리(HBM, GDDR)로 구성된 영역입니다. 호스트로부터 명령을 받아 대규모 병렬 연산을 수행하는 일꾼입니다.
    

가장 중요한 사실은

**호스트와 디바이스는 물리적으로 분리된 메모리 공간을 사용한다**는 것입니다. CPU는 GPU 메모리에 직접 접근할 수 없으며, 그 반대도 마찬가지입니다. 이 둘 사이의 데이터 통신은 상대적으로 매우 느린

**PCIe 버스**를 통해 이루어집니다.

**2. `cudaMemcpy`: 비싼 재료 운송 과정**

[![](images/Pasted%20image%2020250926155932.png)]

GPU가 연산을 하려면 필요한 데이터가 GPU 메모리 안에 있어야 합니다. 이를 위해 우리는

`cudaMemcpy`라는 함수를 사용하여 호스트 메모리의 데이터를 디바이스 메모리로 복사합니다. 연산이 끝난 결과 또한 같은 함수를 통해 다시 호스트로 가져와야 합니다.

문제는 이 과정이 GPU의 연산 속도에 비해 터무니없이 느리다는 것입니다. 마치 F1 경주용 자동차에 필요한 연료를 자전거로 배달하는 것과 같습니다. 아무리 연산(요리)이 빨라도 재료가 도착하지 않으면 GPU는 아무것도 못 하고 놀게 됩니다.

**3. 커널 실행: GPU에게 일을 시키는 방법**

`myKernel<<<grid, block>>>(...)` 이 독특한 문법은 호스트(CPU)가 디바이스(GPU)에게 "이 커널 함수를, 이만큼의 그리드와 블록 크기로 실행해!"라고 명령하는 호출입니다.

이 명령이 떨어지면, 수많은 스레드들이 GPU 위에서 동시에 깨어납니다. 하지만 처음 깨어난 스레드들은 자신이 누구이며, 어떤 데이터를 처리해야 할지 모르는 '익명의 존재'들입니다.

[![](images/Pasted%20image%2020250926160024.png)]

`int index = blockIdx.x * blockDim.x + threadIdx.x;` 커널 함수 안의 이 한 줄 코드는 단순한 인덱스 계산을 넘어, CUDA 프로그래밍 철학의 정수가 담겨 있습니다.

- **정체성의 선언:** 익명의 스레드들이 이 코드를 실행하는 순간, 각자 고유한 `index` 값을 부여받고 비로소 자신이 처리해야 할 데이터가 무엇인지 알게 됩니다. 혼돈 속에서 질서가 부여되는 순간입니다.
    
- **논리와 물리의 다리:** 개발자는 `blockIdx`, `threadIdx` 같은 논리적, 추상적 좌표를 가지고 문제를 설계하지만 , 실제 데이터는 메모리라는 1차원 선형 공간에 저장되어 있습니다. 이 공식은 개발자의 논리적 설계를 실제 물리적 메모리 주소와 연결하는 다리 역할을 합니다.
    
- **추상화의 약속:** 개발자는 복잡한 SM 스케줄링이나 메모리 컨트롤러의 동작을 알 필요 없이, 오직 이 공식 하나만으로 수백만 개의 작업을 하드웨어에 위임할 수 있습니다. "논리적 구조만 알려주면, 효율적인 처리는 하드웨어가 책임지겠다"는 개발자와 하드웨어 사이의 신성한 약속입니다.
    

**4. 프로파일링: CCTV로 주방 엿보기**

말로만 듣던 '병목'을 어떻게 확인할 수 있을까요? 이때 사용하는 도구가 바로

**NVIDIA Nsight Systems**와 같은 프로파일러입니다. 프로파일러는 프로그램이 실행되는 동안 CPU, GPU, 메모리 등 각 요소가 어떤 일을 얼마나 오랫동안 했는지를 시간 축에 따라 상세히 기록합니다.

[![](images/Pasted%20image%2020250926160104.png)]

위 프로파일링 결과를 보면 충격적인 사실을 발견할 수 있습니다.

- **`cudaMemcpy` (빨간색 긴 막대):** 데이터를 전송하는 데 걸리는 시간이 매우 깁니다.
    
- **`vectorAdd` (녹색 짧은 막대):** 실제 GPU가 연산을 수행하는 커널 실행 시간은 눈 깜짝할 사이에 끝납니다.
    
- **GPU의 유휴 시간:** `cudaMemcpy`가 실행되는 동안 GPU의 핵심 연산 유닛(SMs Active 등)은 거의 아무 일도 하지 않고 놀고 있습니다.
    

**결론:** 2강에서는 간단한 벡터 덧셈 예제를 통해 GPU 컴퓨팅의 성능을 저해하는 가장 큰 범인이 '연산'이 아닌 '데이터 전송'임을 눈으로 확인했습니다. 진정한 CUDA 최적화는 커널 코드 자체를 수정하기 이전에, 이 비싼 데이터 전송 비용을 어떻게 줄이고 숨길 것인가에 대한 고민에서 시작됩니다.