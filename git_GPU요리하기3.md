#### 📄 3강 상세 설명: 마법의 작업대, 공유 메모리 - 최고의 요리(커널) 만들기

**목표:** 2강에서 발견한 데이터 전송 병목 현상을 해결하기 위한 두 가지 핵심 최적화 기법을 배웁니다. 특히, GPU 커널(Kernel) 내부의 성능을 극대화하는 비장의 무기, '공유 메모리'의 원리와 활용법을 마스터합니다.

**서론: 진짜 범인은 누구인가?**

2강의 프로파일링을 통해 우리는 GPU가 대부분의 시간을 연산이 아닌 데이터 전송을 '기다리며' 보낸다는 사실을 확인했습니다. 하지만 진짜 범인은 따로 있습니다. 바로 **CPU처럼 생각하는 개발자의 프로그래밍 습관**입니다.

[![](images/Pasted%20image%2020250926160630.png)]

위 코드처럼, CPU 프로그래밍에 익숙한 개발자는 반복문(`for`) 안에서 데이터를 조금씩 GPU로 보내고(`cudaMemcpy`), 계산하고(`addKernel`), 결과를 가져오는 패턴으로 코드를 작성하기 쉽습니다. 이는 마치 덤프트럭을 불러 사과 상자 '하나'를 싣고 보내고, 다음 트럭을 불러 또 '하나'를 보내는 것과 같은 비효율의 극치입니다. GPU라는 강력한 주방은 대부분의 시간을 재료를 기다리며 허비하게 됩니다.

**1. 1단계 최적화: 통신 오버헤드 제거**

[![](images/Pasted%20image%2020250926160739.png)]

첫 번째 해결책은 놀랍도록 간단합니다. 가장 비싼 작업인 '데이터 운반(`cudaMemcpy`)'과 '작업 완료 확인(`cudaDeviceSynchronize`)'을 반복문 밖으로 빼내는 것입니다.

- **최적화 전:** [매번] 운반 → [매번] 계산 → [매번] 운반 → [매번] 대기 (1000번 반복)
    
- **최적화 후:** [단 한 번] 모든 재료 운반 → [계산만 반복] → [단 한 번] 최종 결과 운반
    

코드 몇 줄의 위치를 옮겼을 뿐이지만, 성능에 미치는 영향은 하늘과 땅 차이입니다. 덤프트럭을 한 번만 불러 필요한 사과 1000상자를 모두 주방에 옮겨 놓으면, GPU는 더 이상 기다리지 않고 쉴 새 없이 자신이 가장 잘하는 '계산'에만 집중할 수 있게 됩니다.

[![](images/Pasted%20image%2020250926160900.png)]

프로파일링 결과(녹색 막대)를 보면, `cudaMemcpy` 호출 사이에 GPU가 쉬는 시간 없이 빽빽하게 일하면서 GPU 점유율이 크게 향상된 것을 확인할 수 있습니다.

**2. 2단계 최적화: 비밀 무기, 공유 메모리 (`__shared__`)**

1단계 최적화로 주방(GPU)에 모든 재료(데이터)는 들어왔습니다. 하지만 아직 문제가 남아있습니다. 요리사(스레드)들이 요리를 할 때마다 저 멀리 있는 중앙 창고(전역 메모리)까지 직접 가서 재료를 하나씩 가져오는 상황입니다. 전역 메모리 접근은 여전히 느린 작업입니다.

이때 사용하는 비밀 무기가 바로 **공유 메모리 (`__shared__`)**입니다. 공유 메모리는 SM 내부에 존재하는 초고속 '작업대'와 같습니다.

[![](images/Pasted%20image%2020250926161117.png)]

최적화 과정은 다음과 같습니다.

1. **작업대 선언:** `__shared__` 키워드로 SM 내부에 고속 작업 공간을 선언합니다.
    
2. **협업으로 재료 옮기기:** 같은 블록에 속한 스레드(한 팀의 요리사들)들이 힘을 합쳐, 필요한 데이터를 전역 메모리(중앙 창고)에서 공유 메모리(작업대)로 한 번에 옮겨 담습니다.
    
3. **동기화 (`__syncthreads()`):** 모든 팀원이 재료를 가져올 때까지 잠시 기다립니다. 이 과정을 통해 데이터 정합성을 보장합니다.
    
4. **초고속 연산:** 이제 모든 스레드는 칩 외부의 느린 전역 메모리로 나갈 필요 없이, 바로 옆에 있는 초고속 작업대의 데이터를 사용하여 연산을 수행합니다.
    

[![](images/Pasted%20image%2020250926161230.png)]

이처럼 거대한 문제를 스레드 블록 단위의 '타일(Tile)'로 잘라, 각 타일에 필요한 데이터를 공유 메모리에 올려놓고 처리하는 기법을 **타일링(Tiling)**이라고 합니다. 이를 위해 스레드는 전체 데이터에서의 절대 위치인 **글로벌 인덱스(idx)**와, 블록 내 작업대에서의 상대 위치인 **로컬 인덱스(tid)**를 모두 사용하게 됩니다.

[![](images/Pasted%20image%2020250926161322.png)]

공유 메모리 최적화 후 프로파일을 보면, GPU 점유율이 100%로 유지될 뿐만 아니라 전체 계산 시간(녹색 막대 총 길이) 자체가 줄어든 것을 확인할 수 있습니다. 이는 메모리 계층 구조를 올바르게 활용한 최적화의 힘을 명확히 보여줍니다.

**심화: 왜 최종 결과(변수 C)는 전역 메모리에 저장해야 할까?**

연산 중간 과정은 빠른 공유 메모리를 사용했지만, 최종 결과물은 다시 느린 전역 메모리에 저장해야 합니다. 그 이유는 두 가지입니다.

1. **Host로 통하는 유일한 관문:** CPU(Host)는 오직 `cudaMemcpy`를 통해서만 GPU의 데이터에 접근할 수 있는데, 이 함수는 전역 메모리만을 대상으로 동작합니다.
    
2. **용량의 한계:** 공유 메모리는 수십 KB로 매우 작지만, 최종 결과 데이터는 수백 MB ~ GB 단위로 매우 커서 공유 메모리에 모두 저장하는 것이 불가능합니다. 최종 요리(결과)를 개인 작업대에 모두 쌓아둘 수 없는 것과 같습니다.
    

**결론:** 3강에서는 비효율적인 데이터 전송 패턴을 개선하고, 공유 메모리라는 마법의 작업대를 활용하여 커널 내부의 연산 속도를 극대화하는 2단계 최적화 과정을 살펴보았습니다. 이 두 가지 기법만으로도 CUDA 프로그램의 성능을 몇 배에서 몇십 배까지 향상시킬 수 있습니다.